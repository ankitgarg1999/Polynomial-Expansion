{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SfXgbvcoByRc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E9x6VPdVPG-O"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch. cuda. is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3qCSUYvACAs7"
      },
      "outputs": [],
      "source": [
        "TrainUrl = \"https://scale-static-assets.s3-us-west-2.amazonaws.com/ml-interview/expand/train.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Kd6QvVQbCCey"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(TrainUrl, sep='=', header=None, names = [\"Input\", \"Output\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BczN5v6CU-VI",
        "outputId": "644d66b0-ca8c-488b-ea40-adeafb90a62f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Input              Output\n",
            "0         (7-3*z)*(-5*z-9)      15*z**2-8*z-63\n",
            "1                  -9*s**2             -9*s**2\n",
            "2            (2-2*n)*(n-1)       -2*n**2+4*n-2\n",
            "3                     x**2                x**2\n",
            "4             (4-x)*(x-23)       -x**2+27*x-92\n",
            "...                    ...                 ...\n",
            "999995   (2*k+14)*(7*k-10)    14*k**2+78*k-140\n",
            "999996      (t-7)*(3*t+28)      3*t**2+7*t-196\n",
            "999997   (10-2*i)*(6*i-10)   -12*i**2+80*i-100\n",
            "999998  (-5*k-31)*(8*k+12)  -40*k**2-308*k-372\n",
            "999999   (-8*i-13)*(3*i+4)    -24*i**2-71*i-52\n",
            "\n",
            "[1000000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7JV2x4HBCPNO"
      },
      "outputs": [],
      "source": [
        "# input sequence as list of strings format\n",
        "data_list_input = data['Input'].values.tolist()\n",
        "# output sequence as list of strings format\n",
        "data_list_output = data['Output'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9n-vKOTkCGAA"
      },
      "outputs": [],
      "source": [
        "# size of the dataset\n",
        "size = data.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-82WBC6HCJor"
      },
      "outputs": [],
      "source": [
        "# string to index mapping\n",
        "stoi = {}\n",
        "# index to string mapping\n",
        "itos = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y7R185zvCMYR"
      },
      "outputs": [],
      "source": [
        "# Padding token\n",
        "stoi['<PAD>'] = 0\n",
        "itos[0] = '<PAD>'\n",
        "# Out of vocabulary token\n",
        "stoi['<OOV>'] = 1\n",
        "itos[1] = '<OOV>'\n",
        "# Start of sentence token\n",
        "stoi['<SOS>'] = 2\n",
        "itos[2] = '<SOS>'\n",
        "# End of sentence token\n",
        "stoi['<EOS>'] = 3\n",
        "itos[3] = '<EOS>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jNUbRNzQCR-w"
      },
      "outputs": [],
      "source": [
        "# traverse data and build vocabulary\n",
        "for i in range(0, size):\n",
        "  \n",
        "  for j in range(0, len(data_list_input[i])):\n",
        "    x = stoi.get(data_list_input[i][j], -1)\n",
        "    index = len(stoi.keys())\n",
        "    if (x == -1):\n",
        "      stoi[data_list_input[i][j]] = index\n",
        "      itos[index] = data_list_input[i][j]\n",
        "\n",
        "  for j in range(0, len(data_list_output[i])):\n",
        "    x = stoi.get(data_list_output[i][j], -1)\n",
        "    index = len(stoi.keys())\n",
        "    if (x == -1):\n",
        "      stoi[data_list_output[i][j]] = index\n",
        "      itos[index] = data_list_output[i][j]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze vocabulary\n",
        "print(stoi.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K39dL0zHVvmn",
        "outputId": "acfdaa4c-9583-4955-e03c-e2f88c2b8c7f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['<PAD>', '<OOV>', '<SOS>', '<EOS>', '(', '7', '-', '3', '*', 'z', ')', '5', '9', '1', '2', '8', '6', 's', 'n', '+', '4', 'x', 'c', '0', 'k', 'o', 'j', 'h', 'y', 'i', 't', 'a'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MT7QHB51CUsB"
      },
      "outputs": [],
      "source": [
        "# input sequence is append with <EOS> in the end\n",
        "# output sequence is appended with <SOS> in start and <EOS> in the end\n",
        "# both input and output sequences are appended with <PAD> to get the same length which is max_seq_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3o6CehEMCnW-"
      },
      "outputs": [],
      "source": [
        "# Rounding off max seq len to 32 after including <SOS>, <EOS> and <PAD> \n",
        "max_seq_len = 32\n",
        "embedding_dimension = 256\n",
        "b_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r4qEzywTCazd"
      },
      "outputs": [],
      "source": [
        "# Convert input data from string to integer format\n",
        "# with the help of stoi vocabulary built previously\n",
        "\n",
        "data_list_input_index = []\n",
        "data_list_input_padding = []\n",
        "data_list_output_index = []\n",
        "data_list_output_padding = []\n",
        "\n",
        "for i in range(0, len(data_list_input)):\n",
        "\n",
        "  x = []\n",
        "  x_pad = []\n",
        "  \n",
        "  for j in range(0, len(data_list_input[i])):\n",
        "    x.append(stoi[data_list_input[i][j]])\n",
        "    x_pad.append(False)\n",
        "\n",
        "  # Append <EOS> token in the end\n",
        "  x.append(stoi['<EOS>'])\n",
        "  x_pad.append(False)\n",
        "\n",
        "  while(len(x)<max_seq_len):\n",
        "    x.append(stoi['<PAD>'])\n",
        "    x_pad.append(True)\n",
        "\n",
        "  data_list_input_index.append(x)\n",
        "  data_list_input_padding.append(x_pad)\n",
        "\n",
        "  y = []\n",
        "  y_pad = []\n",
        "\n",
        "  # Append <SOS> token in the end\n",
        "  y.append(stoi['<SOS>'])\n",
        "  y_pad.append(False)\n",
        "\n",
        "  for j in range(0, len(data_list_output[i])):\n",
        "    y.append(stoi[data_list_output[i][j]])\n",
        "    y_pad.append(False)\n",
        "\n",
        "  # Append <EOS> token in the end\n",
        "  y.append(stoi['<EOS>'])\n",
        "  y_pad.append(False)\n",
        "  while(len(y)<max_seq_len):\n",
        "    y.append(stoi['<PAD>'])\n",
        "    y_pad.append(True)\n",
        "  data_list_output_index.append(y)\n",
        "  data_list_output_padding.append(y_pad)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TGkdR7D0KYKe"
      },
      "outputs": [],
      "source": [
        "# Define custom dataset from PyTorch Dataset class\n",
        "class MyDataset (Dataset):\n",
        "\n",
        "  def __init__(self, data_list_input_index, data_list_output_index, data_list_input_padding, data_list_output_padding):\n",
        "    # Lists of lists\n",
        "    self.X = data_list_input_index \n",
        "    self.Y = data_list_output_index\n",
        "    self.X_pad_mask = data_list_input_padding\n",
        "    self.Y_pad_mask = data_list_output_padding\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.X[idx], dtype=torch.int), torch.tensor(self.Y[idx], dtype=torch.int) , torch.tensor(self.X_pad_mask[idx], dtype=torch.bool), torch.tensor(self.Y_pad_mask[idx], dtype=torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6l5QpPvjNPLo"
      },
      "outputs": [],
      "source": [
        "# Train, Validation and Test data sets\n",
        "# 60/20/20 split of the original data\n",
        "train_data = MyDataset(data_list_input_index[0:(int)(0.6*size)], data_list_output_index[0:(int)(0.6*size)], data_list_input_padding[0:(int)(0.6*size)], data_list_output_padding[0:(int)(0.6*size)])\n",
        "val_data = MyDataset(data_list_input_index[(int)(0.6*size):(int)(0.8*size)], data_list_output_index[(int)(0.6*size):(int)(0.8*size)], data_list_input_padding[(int)(0.6*size):(int)(0.8*size)], data_list_output_padding[(int)(0.6*size):(int)(0.8*size)])\n",
        "test_data = MyDataset(data_list_input_index[(int)(0.8*size):], data_list_output_index[(int)(0.8*size):], data_list_input_padding[(int)(0.8*size):], data_list_output_padding[(int)(0.8*size):])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9BsK31y9S0CZ"
      },
      "outputs": [],
      "source": [
        "# Train, Validation and Test data loaders\n",
        "train_loader = DataLoader(train_data, batch_size = b_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size = b_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size = b_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ka6mBHdFCpTi"
      },
      "outputs": [],
      "source": [
        "# Positional Embeddings - in accordance with Attention Is All You Need paper\n",
        "pos_embeddings = np.zeros((max_seq_len, embedding_dimension), dtype=float)\n",
        "\n",
        "for i in range(0, max_seq_len):\n",
        "  for j in range(0, embedding_dimension):\n",
        "    omega = 1 / (10000**((j - (j%2))/embedding_dimension))\n",
        "    if (j % 2 == 0):\n",
        "      pos_embeddings[i][j] = math.sin(i*omega)\n",
        "    else:\n",
        "      pos_embeddings[i][j] = math.cos(i*omega)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OYvg_aX1cCKS"
      },
      "outputs": [],
      "source": [
        "pos_embeddings = ((torch.from_numpy(pos_embeddings)).float()).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xq6jDx0ACrx_"
      },
      "outputs": [],
      "source": [
        "# Embeddings layer - used for both transformer encode and decoder\n",
        "class Embed_Model(nn.Module):\n",
        "\n",
        "  def __init__(self, number_embeddings, embedding_dimension, padding_index, pos_embeddings):\n",
        "        \n",
        "    super(Embed_Model, self).__init__()\n",
        "\n",
        "    self.number_embeddings = number_embeddings\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "    self.padding_index = padding_index\n",
        "    self.pos_embeddings = pos_embeddings\n",
        "\n",
        "    self.embedding_layer = nn.Embedding(num_embeddings=self.number_embeddings, embedding_dim=self.embedding_dimension, padding_idx=self.padding_index)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Input dimension - (batch size, seq length)\n",
        "    # Sequence length is set as max_seq_length. Padding tokens are added in the end\n",
        "    output = self.embedding_layer(x) + self.pos_embeddings\n",
        "    # Output dimension - (batch size, seq length, embedding dim)\n",
        "    # Add the positional embeddings and return the output\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TWaWzaWvCwxp"
      },
      "outputs": [],
      "source": [
        "# Complete Model\n",
        "# Contains both embedding layer and transformer model (as implemented by PyTorch)\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self, number_embeddings, embedding_dimension, feedforward_dimension, padding_index, num_heads, encoder_number_layers, decoder_number_layers, pos_embeddings):\n",
        "        \n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.embeddings = Embed_Model(number_embeddings, embedding_dimension, padding_index, pos_embeddings)\n",
        "    self.transformer = nn.Transformer(d_model=embedding_dimension, nhead=num_heads, num_encoder_layers=encoder_number_layers, \n",
        "                                      num_decoder_layers=decoder_number_layers, dim_feedforward=feedforward_dimension, batch_first=True)\n",
        "    self.fc = nn.Linear(embedding_dimension, number_embeddings)\n",
        "\n",
        "# src: (N, S, E) if batch_first=True.\n",
        "\n",
        "# tgt: (N, T, E) if batch_first=True.\n",
        "\n",
        "# tgt_mask: lower triangular matrix marked with False - (T, T)\n",
        "# [False, True]\n",
        "# [False, False]\n",
        "\n",
        "# src_key_padding_mask: mark true for padded tokens - (N, S)\n",
        "\n",
        "# tgt_key_padding_mask: mark true for padded tokens - (N, T)\n",
        "\n",
        "# memory_key_padding_mask: kept same as source key padding mask - (N, S)\n",
        "\n",
        "  def forward(self, src, tgt, tgt_mask, src_key_padding_mask, tgt_key_padding_mask):\n",
        "    \n",
        "    src_embedding = self.embeddings(src)\n",
        "    tgt_embedding = self.embeddings(tgt)\n",
        "    output = self.transformer(src_embedding, tgt_embedding, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n",
        "    output = self.fc(output)\n",
        "    output = F.softmax(output, dim=2)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9GJ6zILHY-NU"
      },
      "outputs": [],
      "source": [
        "# All the hyperparameters (no of encoders, no of decoders, feed forward dimension, no attention heads) \n",
        "# were reduced by half to make the number of trainable parameters approximately 5 Million\n",
        "number_embeddings = len(stoi.keys())\n",
        "embedding_dimension = 256\n",
        "padding_index = 0\n",
        "num_heads = 4\n",
        "encoder_number_layers = 3\n",
        "decoder_number_layers = 3\n",
        "feedforward_dimension = 1024\n",
        "\n",
        "neural_net = Model(number_embeddings, embedding_dimension, feedforward_dimension, padding_index, num_heads, encoder_number_layers, decoder_number_layers, pos_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iB30Zcn1PPCc"
      },
      "outputs": [],
      "source": [
        "neural_net = neural_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HwysGoxyR5er"
      },
      "outputs": [],
      "source": [
        "# SGD optimizer\n",
        "optm = optim.SGD(neural_net.parameters(), lr = 0.1, momentum=0.9)\n",
        "# Exponential Decay Scheduler\n",
        "scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optm, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yMJUM-8pyX4I"
      },
      "outputs": [],
      "source": [
        "# To prevent decoder from cheating (so it doesn't attend to future positions)\n",
        "# Lower Triangular Matrix marked as false\n",
        "tgt_mask_tensor = torch.zeros((max_seq_len, max_seq_len), dtype=torch.bool)\n",
        "\n",
        "for i in range(0, max_seq_len):\n",
        "  for j in range(0, max_seq_len):\n",
        "\n",
        "    if (j<=i):\n",
        "      tgt_mask_tensor[i][j] = False\n",
        "\n",
        "    else:\n",
        "      tgt_mask_tensor[i][j] = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQaiAXF0SeIl",
        "outputId": "b6a5cde0-c1d8-4661-b2b8-aa58ac196809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** Epoch:  1  ********************\n",
            "Training Loss = tensor(0.2963, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.1274, device='cuda:0')\n",
            "*************** Epoch:  2  ********************\n",
            "Training Loss = tensor(0.0993, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0831, device='cuda:0')\n",
            "*************** Epoch:  3  ********************\n",
            "Training Loss = tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0534, device='cuda:0')\n",
            "*************** Epoch:  4  ********************\n",
            "Training Loss = tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0424, device='cuda:0')\n",
            "*************** Epoch:  5  ********************\n",
            "Training Loss = tensor(0.0370, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0340, device='cuda:0')\n",
            "*************** Epoch:  6  ********************\n",
            "Training Loss = tensor(0.0308, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0299, device='cuda:0')\n",
            "*************** Epoch:  7  ********************\n",
            "Training Loss = tensor(0.0263, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0254, device='cuda:0')\n",
            "*************** Epoch:  8  ********************\n",
            "Training Loss = tensor(0.0227, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0216, device='cuda:0')\n",
            "*************** Epoch:  9  ********************\n",
            "Training Loss = tensor(0.0200, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0188, device='cuda:0')\n",
            "*************** Epoch:  10  ********************\n",
            "Training Loss = tensor(0.0176, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0178, device='cuda:0')\n",
            "*************** Epoch:  11  ********************\n",
            "Training Loss = tensor(0.0158, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0157, device='cuda:0')\n",
            "*************** Epoch:  12  ********************\n",
            "Training Loss = tensor(0.0142, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0149, device='cuda:0')\n",
            "*************** Epoch:  13  ********************\n",
            "Training Loss = tensor(0.0129, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0132, device='cuda:0')\n",
            "*************** Epoch:  14  ********************\n",
            "Training Loss = tensor(0.0119, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0123, device='cuda:0')\n",
            "*************** Epoch:  15  ********************\n",
            "Training Loss = tensor(0.0110, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0111, device='cuda:0')\n",
            "*************** Epoch:  16  ********************\n",
            "Training Loss = tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0104, device='cuda:0')\n",
            "*************** Epoch:  17  ********************\n",
            "Training Loss = tensor(0.0095, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0096, device='cuda:0')\n",
            "*************** Epoch:  18  ********************\n",
            "Training Loss = tensor(0.0089, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0091, device='cuda:0')\n",
            "*************** Epoch:  19  ********************\n",
            "Training Loss = tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0088, device='cuda:0')\n",
            "*************** Epoch:  20  ********************\n",
            "Training Loss = tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>); Val Loss = tensor(0.0082, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Model Training with Early stopping as the Regularizer\n",
        "# However I did not see increase in validation loss for first 20 epochs\n",
        "# and hence we didn't have to use early stopping\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for i in range(0, epochs):\n",
        "\n",
        "  print(\"*************** Epoch: \", i+1, \" ********************\")\n",
        "\n",
        "  loss_t = 0\n",
        "\n",
        "  for data in train_loader:\n",
        "\n",
        "    optm.zero_grad()\n",
        "\n",
        "    src = data[0].to(device)\n",
        "    tgt = data[1].to(device)\n",
        "    src_mask_padding = data[2].to(device)\n",
        "    tgt_mask_padding = data[3].to(device)\n",
        "    tgt_mask_tensor_input = tgt_mask_tensor.to(device)\n",
        "\n",
        "    output = neural_net(src, tgt, tgt_mask_tensor_input, src_mask_padding, tgt_mask_padding)\n",
        "\n",
        "    # shifted output\n",
        "    tgt = torch.roll(tgt, -1, 1)\n",
        "    tgt = tgt.view(-1)\n",
        "    # mask <pad> tokens and <sos> as these won't contribute to\n",
        "    # the cross entropy loss\n",
        "    mask = torch.logical_not(torch.logical_or(tgt == 0, tgt == 2))\n",
        "    mask_f = mask.float()\n",
        "\n",
        "    output = output.view(-1, number_embeddings)\n",
        "    ll = -torch.log(output[range(output.shape[0]), tgt.long()])\n",
        "    ll = (ll * mask_f).masked_select(mask).mean()\n",
        "    ll.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(neural_net.parameters(), 0.5)\n",
        "    optm.step()\n",
        "\n",
        "    loss_t = loss_t + (ll*b_size)\n",
        "\n",
        "  loss_t = loss_t/600000\n",
        "  \n",
        "  loss_v = 0\n",
        "  with torch.no_grad():\n",
        "    for data in val_loader:\n",
        "      \n",
        "      src = data[0].to(device)\n",
        "      tgt = data[1].to(device)\n",
        "      src_mask_padding = data[2].to(device)\n",
        "      tgt_mask_padding = data[3].to(device)\n",
        "      tgt_mask_tensor_input = tgt_mask_tensor.to(device)\n",
        "\n",
        "      output = neural_net(src, tgt, tgt_mask_tensor_input, src_mask_padding, tgt_mask_padding)\n",
        "\n",
        "      # shifted output\n",
        "      tgt = torch.roll(tgt, -1, 1)\n",
        "      tgt = tgt.view(-1)\n",
        "      # mask <pad> tokens and <sos> as these won't contribute to\n",
        "      # the cross entropy loss\n",
        "      mask = torch.logical_not(torch.logical_or(tgt == 0, tgt == 2))\n",
        "      mask_f = mask.float()\n",
        "\n",
        "      output = output.view(-1, number_embeddings)\n",
        "      ll = -torch.log(output[range(output.shape[0]), tgt.long()])\n",
        "      ll = (ll * mask_f).masked_select(mask).mean()\n",
        "\n",
        "      loss_v = loss_v + (ll*b_size)\n",
        "\n",
        "  loss_v = loss_v/200000\n",
        "\n",
        "  print(\"Training Loss = \"+str(loss_t)+\"; Val Loss = \"+str(loss_v))\n",
        "  scheduler1.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/My Drive/model2.pt\"\n",
        "torch.save(neural_net, model_path)"
      ],
      "metadata": {
        "id": "Zo3kZDI0a5hH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net_loaded = Model(number_embeddings, embedding_dimension, feedforward_dimension, padding_index, num_heads, encoder_number_layers, decoder_number_layers, pos_embeddings)\n",
        "neural_net_loaded = torch.load(\"/content/drive/My Drive/model2.pt\")\n",
        "neural_net_loaded.eval()\n",
        "neural_net_loaded = neural_net_loaded.to(device)"
      ],
      "metadata": {
        "id": "8JrYoyABmUyy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KARbkmVbvmJv"
      },
      "outputs": [],
      "source": [
        "# Check Train and Validation accuracy\n",
        "# Different from Test accuracy since we know the correct output\n",
        "# Hence accuracy check is done with teacher forcing\n",
        "def pred_accuracy(data_loader, neural_net):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "\n",
        "      src = data[0].to(device)\n",
        "      tgt = data[1].to(device)\n",
        "      src_mask_padding = data[2].to(device)\n",
        "      tgt_mask_padding = data[3].to(device)\n",
        "      tgt_mask_tensor_input = tgt_mask_tensor.to(device)\n",
        "\n",
        "      # shape [batch, seq, num_embeddings]\n",
        "      output = neural_net(src, tgt, tgt_mask_tensor_input, src_mask_padding, tgt_mask_padding)\n",
        "\n",
        "      # shifted output\n",
        "      tgt = torch.roll(tgt, -1, 1)\n",
        "      # mask <pad> tokens and <sos> as these won't contribute to our accuracy prediction\n",
        "      mask = torch.logical_not(torch.logical_or(tgt == 0, tgt == 2))\n",
        "\n",
        "      # resulting preductions is [batch, seq]\n",
        "      predictions = torch.argmax(output, 2)\n",
        "\n",
        "      for i in range(0, src.shape[0]):\n",
        "        tensor1 = tgt[i].masked_select(mask[i])\n",
        "        tensor2 = predictions[i].masked_select(mask[i])\n",
        "        total += 1\n",
        "        if (torch.equal(tensor1, tensor2)):\n",
        "          correct += 1\n",
        "      \n",
        "  return (correct/total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtUaiBwBma70",
        "outputId": "46fda8e1-42ec-4768-880b-9115a78bccd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9926766666666667\n"
          ]
        }
      ],
      "source": [
        "print(pred_accuracy(train_loader, neural_net_loaded))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_accuracy(val_loader, neural_net_loaded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUeGYjgLtsOF",
        "outputId": "6c9c78f1-6f13-4f36-a897-1e4e67d8f896"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.99127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qrfrl2xXBEAF"
      },
      "outputs": [],
      "source": [
        "# Check Test set accuracy\n",
        "def test_pred_accuracy(data_loader, neural_net, batch_size=b_size):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "\n",
        "      src = data[0].to(device)\n",
        "      tgt = data[1].to(device)\n",
        "      src_mask_padding = data[2].to(device)\n",
        "      tgt_mask_padding = data[3].to(device)\n",
        "      tgt_mask_tensor_input = tgt_mask_tensor.to(device)\n",
        "\n",
        "      # shape [1, seq, num_embeddings]\n",
        "      # need to decode token by token\n",
        "\n",
        "      # start with a <SOS> token which corresponds with 2\n",
        "      input_tgt = (torch.zeros((batch_size, max_seq_len)).long()).to(device) \n",
        "      input_tgt[:, 0] = stoi['<SOS>']\n",
        "      output_tgt = torch.zeros((batch_size, max_seq_len)).long() \n",
        "\n",
        "      for i in range(0, max_seq_len):\n",
        "        output = neural_net(src, input_tgt, tgt_mask_tensor_input, src_mask_padding, tgt_mask_padding)\n",
        "        output_tgt = torch.argmax(output, 2)\n",
        "        if (i+1<max_seq_len):\n",
        "          input_tgt[:, i+1] = output_tgt[:, i]\n",
        "\n",
        "      # shifted output\n",
        "      tgt = torch.roll(tgt, -1, 1)\n",
        "      # mask <pad> tokens and <sos> as these won't contribute to our accuracy prediction\n",
        "      mask = torch.logical_not(torch.logical_or(tgt == 0, tgt == 2))\n",
        "\n",
        "      for i in range(0, src.shape[0]):\n",
        "        tensor1 = tgt[i].masked_select(mask[i])\n",
        "        tensor2 = output_tgt[i].masked_select(mask[i])\n",
        "        total += 1\n",
        "        if (torch.equal(tensor1, tensor2)):\n",
        "          correct += 1\n",
        "      \n",
        "  return (correct/total)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_pred_accuracy(test_loader, neural_net_loaded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl-IifK7FoM8",
        "outputId": "02ae1255-0c18-498b-e1f5-f261449a5cf8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.99145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Analysis**\n",
        "Type of layers, no of trainable parameters etc."
      ],
      "metadata": {
        "id": "B9vIOmaQGs-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(neural_net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1TLNRMjhg4b",
        "outputId": "5dfc26de-d2bf-4ae2-f3fc-cc9c67bda500"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (embeddings): Embed_Model(\n",
            "    (embedding_layer): Embedding(32, 256, padding_idx=0)\n",
            "  )\n",
            "  (transformer): Transformer(\n",
            "    (encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=32, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GzRln78Jtz59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e5ef6a-fc47-4fbe-d191-c11250c91bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5547040\n"
          ]
        }
      ],
      "source": [
        "# Source: https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n",
        "trainable_params = sum(parameter.numel() for parameter in neural_net.parameters() if parameter.requires_grad)\n",
        "print(trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params+=params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "    \n",
        "count_parameters(neural_net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd3CE-ediNsR",
        "outputId": "6fcc19f7-26d3-48b5-e8d9-1ebfd1fe35a0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------+------------+\n",
            "|                           Modules                           | Parameters |\n",
            "+-------------------------------------------------------------+------------+\n",
            "|              embeddings.embedding_layer.weight              |    8192    |\n",
            "|    transformer.encoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
            "|     transformer.encoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
            "|    transformer.encoder.layers.0.self_attn.out_proj.weight   |   65536    |\n",
            "|     transformer.encoder.layers.0.self_attn.out_proj.bias    |    256     |\n",
            "|         transformer.encoder.layers.0.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.0.linear1.bias          |    1024    |\n",
            "|         transformer.encoder.layers.0.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.0.linear2.bias          |    256     |\n",
            "|          transformer.encoder.layers.0.norm1.weight          |    256     |\n",
            "|           transformer.encoder.layers.0.norm1.bias           |    256     |\n",
            "|          transformer.encoder.layers.0.norm2.weight          |    256     |\n",
            "|           transformer.encoder.layers.0.norm2.bias           |    256     |\n",
            "|    transformer.encoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
            "|     transformer.encoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
            "|    transformer.encoder.layers.1.self_attn.out_proj.weight   |   65536    |\n",
            "|     transformer.encoder.layers.1.self_attn.out_proj.bias    |    256     |\n",
            "|         transformer.encoder.layers.1.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.1.linear1.bias          |    1024    |\n",
            "|         transformer.encoder.layers.1.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.1.linear2.bias          |    256     |\n",
            "|          transformer.encoder.layers.1.norm1.weight          |    256     |\n",
            "|           transformer.encoder.layers.1.norm1.bias           |    256     |\n",
            "|          transformer.encoder.layers.1.norm2.weight          |    256     |\n",
            "|           transformer.encoder.layers.1.norm2.bias           |    256     |\n",
            "|    transformer.encoder.layers.2.self_attn.in_proj_weight    |   196608   |\n",
            "|     transformer.encoder.layers.2.self_attn.in_proj_bias     |    768     |\n",
            "|    transformer.encoder.layers.2.self_attn.out_proj.weight   |   65536    |\n",
            "|     transformer.encoder.layers.2.self_attn.out_proj.bias    |    256     |\n",
            "|         transformer.encoder.layers.2.linear1.weight         |   262144   |\n",
            "|          transformer.encoder.layers.2.linear1.bias          |    1024    |\n",
            "|         transformer.encoder.layers.2.linear2.weight         |   262144   |\n",
            "|          transformer.encoder.layers.2.linear2.bias          |    256     |\n",
            "|          transformer.encoder.layers.2.norm1.weight          |    256     |\n",
            "|           transformer.encoder.layers.2.norm1.bias           |    256     |\n",
            "|          transformer.encoder.layers.2.norm2.weight          |    256     |\n",
            "|           transformer.encoder.layers.2.norm2.bias           |    256     |\n",
            "|               transformer.encoder.norm.weight               |    256     |\n",
            "|                transformer.encoder.norm.bias                |    256     |\n",
            "|    transformer.decoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
            "|     transformer.decoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
            "|    transformer.decoder.layers.0.self_attn.out_proj.weight   |   65536    |\n",
            "|     transformer.decoder.layers.0.self_attn.out_proj.bias    |    256     |\n",
            "|  transformer.decoder.layers.0.multihead_attn.in_proj_weight |   196608   |\n",
            "|   transformer.decoder.layers.0.multihead_attn.in_proj_bias  |    768     |\n",
            "| transformer.decoder.layers.0.multihead_attn.out_proj.weight |   65536    |\n",
            "|  transformer.decoder.layers.0.multihead_attn.out_proj.bias  |    256     |\n",
            "|         transformer.decoder.layers.0.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.0.linear1.bias          |    1024    |\n",
            "|         transformer.decoder.layers.0.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.0.linear2.bias          |    256     |\n",
            "|          transformer.decoder.layers.0.norm1.weight          |    256     |\n",
            "|           transformer.decoder.layers.0.norm1.bias           |    256     |\n",
            "|          transformer.decoder.layers.0.norm2.weight          |    256     |\n",
            "|           transformer.decoder.layers.0.norm2.bias           |    256     |\n",
            "|          transformer.decoder.layers.0.norm3.weight          |    256     |\n",
            "|           transformer.decoder.layers.0.norm3.bias           |    256     |\n",
            "|    transformer.decoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
            "|     transformer.decoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
            "|    transformer.decoder.layers.1.self_attn.out_proj.weight   |   65536    |\n",
            "|     transformer.decoder.layers.1.self_attn.out_proj.bias    |    256     |\n",
            "|  transformer.decoder.layers.1.multihead_attn.in_proj_weight |   196608   |\n",
            "|   transformer.decoder.layers.1.multihead_attn.in_proj_bias  |    768     |\n",
            "| transformer.decoder.layers.1.multihead_attn.out_proj.weight |   65536    |\n",
            "|  transformer.decoder.layers.1.multihead_attn.out_proj.bias  |    256     |\n",
            "|         transformer.decoder.layers.1.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.1.linear1.bias          |    1024    |\n",
            "|         transformer.decoder.layers.1.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.1.linear2.bias          |    256     |\n",
            "|          transformer.decoder.layers.1.norm1.weight          |    256     |\n",
            "|           transformer.decoder.layers.1.norm1.bias           |    256     |\n",
            "|          transformer.decoder.layers.1.norm2.weight          |    256     |\n",
            "|           transformer.decoder.layers.1.norm2.bias           |    256     |\n",
            "|          transformer.decoder.layers.1.norm3.weight          |    256     |\n",
            "|           transformer.decoder.layers.1.norm3.bias           |    256     |\n",
            "|    transformer.decoder.layers.2.self_attn.in_proj_weight    |   196608   |\n",
            "|     transformer.decoder.layers.2.self_attn.in_proj_bias     |    768     |\n",
            "|    transformer.decoder.layers.2.self_attn.out_proj.weight   |   65536    |\n",
            "|     transformer.decoder.layers.2.self_attn.out_proj.bias    |    256     |\n",
            "|  transformer.decoder.layers.2.multihead_attn.in_proj_weight |   196608   |\n",
            "|   transformer.decoder.layers.2.multihead_attn.in_proj_bias  |    768     |\n",
            "| transformer.decoder.layers.2.multihead_attn.out_proj.weight |   65536    |\n",
            "|  transformer.decoder.layers.2.multihead_attn.out_proj.bias  |    256     |\n",
            "|         transformer.decoder.layers.2.linear1.weight         |   262144   |\n",
            "|          transformer.decoder.layers.2.linear1.bias          |    1024    |\n",
            "|         transformer.decoder.layers.2.linear2.weight         |   262144   |\n",
            "|          transformer.decoder.layers.2.linear2.bias          |    256     |\n",
            "|          transformer.decoder.layers.2.norm1.weight          |    256     |\n",
            "|           transformer.decoder.layers.2.norm1.bias           |    256     |\n",
            "|          transformer.decoder.layers.2.norm2.weight          |    256     |\n",
            "|           transformer.decoder.layers.2.norm2.bias           |    256     |\n",
            "|          transformer.decoder.layers.2.norm3.weight          |    256     |\n",
            "|           transformer.decoder.layers.2.norm3.bias           |    256     |\n",
            "|               transformer.decoder.norm.weight               |    256     |\n",
            "|                transformer.decoder.norm.bias                |    256     |\n",
            "|                          fc.weight                          |    8192    |\n",
            "|                           fc.bias                           |     32     |\n",
            "+-------------------------------------------------------------+------------+\n",
            "Total Trainable Params: 5547040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5547040"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}