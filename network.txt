+-------------------------------------------------------------+------------+
|                           Modules                           | Parameters |
+-------------------------------------------------------------+------------+
|              embeddings.embedding_layer.weight              |    8192    |
|    transformer.encoder.layers.0.self_attn.in_proj_weight    |   196608   |
|     transformer.encoder.layers.0.self_attn.in_proj_bias     |    768     |
|    transformer.encoder.layers.0.self_attn.out_proj.weight   |   65536    |
|     transformer.encoder.layers.0.self_attn.out_proj.bias    |    256     |
|         transformer.encoder.layers.0.linear1.weight         |   262144   |
|          transformer.encoder.layers.0.linear1.bias          |    1024    |
|         transformer.encoder.layers.0.linear2.weight         |   262144   |
|          transformer.encoder.layers.0.linear2.bias          |    256     |
|          transformer.encoder.layers.0.norm1.weight          |    256     |
|           transformer.encoder.layers.0.norm1.bias           |    256     |
|          transformer.encoder.layers.0.norm2.weight          |    256     |
|           transformer.encoder.layers.0.norm2.bias           |    256     |
|    transformer.encoder.layers.1.self_attn.in_proj_weight    |   196608   |
|     transformer.encoder.layers.1.self_attn.in_proj_bias     |    768     |
|    transformer.encoder.layers.1.self_attn.out_proj.weight   |   65536    |
|     transformer.encoder.layers.1.self_attn.out_proj.bias    |    256     |
|         transformer.encoder.layers.1.linear1.weight         |   262144   |
|          transformer.encoder.layers.1.linear1.bias          |    1024    |
|         transformer.encoder.layers.1.linear2.weight         |   262144   |
|          transformer.encoder.layers.1.linear2.bias          |    256     |
|          transformer.encoder.layers.1.norm1.weight          |    256     |
|           transformer.encoder.layers.1.norm1.bias           |    256     |
|          transformer.encoder.layers.1.norm2.weight          |    256     |
|           transformer.encoder.layers.1.norm2.bias           |    256     |
|    transformer.encoder.layers.2.self_attn.in_proj_weight    |   196608   |
|     transformer.encoder.layers.2.self_attn.in_proj_bias     |    768     |
|    transformer.encoder.layers.2.self_attn.out_proj.weight   |   65536    |
|     transformer.encoder.layers.2.self_attn.out_proj.bias    |    256     |
|         transformer.encoder.layers.2.linear1.weight         |   262144   |
|          transformer.encoder.layers.2.linear1.bias          |    1024    |
|         transformer.encoder.layers.2.linear2.weight         |   262144   |
|          transformer.encoder.layers.2.linear2.bias          |    256     |
|          transformer.encoder.layers.2.norm1.weight          |    256     |
|           transformer.encoder.layers.2.norm1.bias           |    256     |
|          transformer.encoder.layers.2.norm2.weight          |    256     |
|           transformer.encoder.layers.2.norm2.bias           |    256     |
|               transformer.encoder.norm.weight               |    256     |
|                transformer.encoder.norm.bias                |    256     |
|    transformer.decoder.layers.0.self_attn.in_proj_weight    |   196608   |
|     transformer.decoder.layers.0.self_attn.in_proj_bias     |    768     |
|    transformer.decoder.layers.0.self_attn.out_proj.weight   |   65536    |
|     transformer.decoder.layers.0.self_attn.out_proj.bias    |    256     |
|  transformer.decoder.layers.0.multihead_attn.in_proj_weight |   196608   |
|   transformer.decoder.layers.0.multihead_attn.in_proj_bias  |    768     |
| transformer.decoder.layers.0.multihead_attn.out_proj.weight |   65536    |
|  transformer.decoder.layers.0.multihead_attn.out_proj.bias  |    256     |
|         transformer.decoder.layers.0.linear1.weight         |   262144   |
|          transformer.decoder.layers.0.linear1.bias          |    1024    |
|         transformer.decoder.layers.0.linear2.weight         |   262144   |
|          transformer.decoder.layers.0.linear2.bias          |    256     |
|          transformer.decoder.layers.0.norm1.weight          |    256     |
|           transformer.decoder.layers.0.norm1.bias           |    256     |
|          transformer.decoder.layers.0.norm2.weight          |    256     |
|           transformer.decoder.layers.0.norm2.bias           |    256     |
|          transformer.decoder.layers.0.norm3.weight          |    256     |
|           transformer.decoder.layers.0.norm3.bias           |    256     |
|    transformer.decoder.layers.1.self_attn.in_proj_weight    |   196608   |
|     transformer.decoder.layers.1.self_attn.in_proj_bias     |    768     |
|    transformer.decoder.layers.1.self_attn.out_proj.weight   |   65536    |
|     transformer.decoder.layers.1.self_attn.out_proj.bias    |    256     |
|  transformer.decoder.layers.1.multihead_attn.in_proj_weight |   196608   |
|   transformer.decoder.layers.1.multihead_attn.in_proj_bias  |    768     |
| transformer.decoder.layers.1.multihead_attn.out_proj.weight |   65536    |
|  transformer.decoder.layers.1.multihead_attn.out_proj.bias  |    256     |
|         transformer.decoder.layers.1.linear1.weight         |   262144   |
|          transformer.decoder.layers.1.linear1.bias          |    1024    |
|         transformer.decoder.layers.1.linear2.weight         |   262144   |
|          transformer.decoder.layers.1.linear2.bias          |    256     |
|          transformer.decoder.layers.1.norm1.weight          |    256     |
|           transformer.decoder.layers.1.norm1.bias           |    256     |
|          transformer.decoder.layers.1.norm2.weight          |    256     |
|           transformer.decoder.layers.1.norm2.bias           |    256     |
|          transformer.decoder.layers.1.norm3.weight          |    256     |
|           transformer.decoder.layers.1.norm3.bias           |    256     |
|    transformer.decoder.layers.2.self_attn.in_proj_weight    |   196608   |
|     transformer.decoder.layers.2.self_attn.in_proj_bias     |    768     |
|    transformer.decoder.layers.2.self_attn.out_proj.weight   |   65536    |
|     transformer.decoder.layers.2.self_attn.out_proj.bias    |    256     |
|  transformer.decoder.layers.2.multihead_attn.in_proj_weight |   196608   |
|   transformer.decoder.layers.2.multihead_attn.in_proj_bias  |    768     |
| transformer.decoder.layers.2.multihead_attn.out_proj.weight |   65536    |
|  transformer.decoder.layers.2.multihead_attn.out_proj.bias  |    256     |
|         transformer.decoder.layers.2.linear1.weight         |   262144   |
|          transformer.decoder.layers.2.linear1.bias          |    1024    |
|         transformer.decoder.layers.2.linear2.weight         |   262144   |
|          transformer.decoder.layers.2.linear2.bias          |    256     |
|          transformer.decoder.layers.2.norm1.weight          |    256     |
|           transformer.decoder.layers.2.norm1.bias           |    256     |
|          transformer.decoder.layers.2.norm2.weight          |    256     |
|           transformer.decoder.layers.2.norm2.bias           |    256     |
|          transformer.decoder.layers.2.norm3.weight          |    256     |
|           transformer.decoder.layers.2.norm3.bias           |    256     |
|               transformer.decoder.norm.weight               |    256     |
|                transformer.decoder.norm.bias                |    256     |
|                          fc.weight                          |    8192    |
|                           fc.bias                           |     32     |
+-------------------------------------------------------------+------------+
Total Trainable Params: 5547040
5547040

*****************************************************************************************************************************************************************

Model(
  (embeddings): Embed_Model(
    (embedding_layer): Embedding(32, 256, padding_idx=0)
  )
  (transformer): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (fc): Linear(in_features=256, out_features=32, bias=True)
)